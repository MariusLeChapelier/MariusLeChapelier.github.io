---
title: 'About me'
date: 2024-07-22T12:15:22+02:00
draft: false
ShowToc: false
ShowReadingTime: false
ShowBreadCrumbs: false
hidemeta: true
---

## Hi! I'm Marius. ðŸ‘‹

I'm a research engineer at [Inria Paris](https://www.inria.fr/en), where I study human interactions through the prism of AI and, especially, **Natural Langage Processing (NLP)** applied to **human dialogue**. I'm part of the the [Articulab](https://articulab.hcii.cs.cmu.edu/), within the [Almanach](https://almanach.inria.fr/index-en.html) team, our work contributes to theoretical research in cognitive science, linguistics, artificial intelligence and other disciplines.

Dialogue is a complex interaction, as it involves diverse, simultaneous and interdependent actions. These actions constitute parallel signals of different types (voice, gesture, facial expression, etc.), at different scales (turn, sentence, word or even shorter) and in different temporal domains (continuous or discrete).

We generally distinguish 3 types of signals, 3 **modalities** :

- Verbal (semantic information)
- Vocal (prosody, intonation, etc)
- Non-Verbal (gesture, facial expression, etc)

It has been shown in multiple past research that the study of the relationship between these modalities is essential for a better understanding of human interaction, and that their synergy benefits greatly the modeling of such interaction. Our interest is to take forward the recent advances on training multimodal neural networks models to understand or generate human-like interactions.

In this context, I'm currently working on a **real-time multimodal dialogue system** and **embodied conversational agent**. This project is a sequel of Articulab's previous [**SARA**](https://articulab.hcii.cs.cmu.edu/projects/sara/). A dialogue system is a system capable of having a conversation with a human interlocutor. Which implies that it must be able, in real-time, to process the user's multimodal signals : vocal (capturing user's voice with a microphone), verbal (extracting semantic information from user's transcription), non-verbal (capturing gesture, facial expression with a camera), and to generate similar signals through an avatar.

Prior to this, I obtained my [Master's degree in Artificial Intelligence from Sorbonne UniversitÃ©](https://sciences.sorbonne-universite.fr/en/masters/master-computer-science/distributed-agents-robotics-operations-research-interaction), during which I studied the following AI subfields :

- Interactive environments : virtual environments, human-computer interaction, serious games, video games, e-learning, information systems
- Decision : decision theory, preference modeling and learning, multi-objective or multi-agent combinatorial optimization, Bayesian networks
- Robotics and intelligent systems : agent and autonomous robot, multi-agent systems, machine learning
- Operational research : mathematical programming, optimization and complexity, graphs and scheduling
